---
title       : "Getting the most out of R"
author      : "Alex M Chubaty & Eliot McIntire"
date        : "August 25, 2016"
output:
  ioslides_presentation:
    logo: images/predictive_ecology_logo.png
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(eval = FALSE)
knitr::opts_chunk$set(cache = TRUE)
```

## Need for speed

- R is widely seed as being 'slow' (see [julia](http://julialang.org/#high-performance-jit-compiler) web page) 
- But, if you use a few specific tools, then this becomes irrelevant because of the powerful tools in various packages in R
- (as an aside, pure R, when the most efficient vectorized code is used appears to be 1/2x the speed of the most efficient C++ -- see Hadley Wickham's [page on Rcpp](http://adv-r.had.co.nz/Rcpp.html), scroll down to "Vector input, vector output"... ), noting that if it took 10 minutes to write the C++ code, it would have to be 150,000 times faster to make it worth it)

## Key packages for spatial simulation - page 1/2

- `base` package -- everything matrix or vector is 'fast'
- `raster` - for spatially referenced matrices

    - not always fast enough, sometimes we copy the data into a matrix, then manipulate, then return the data to the raster object
    
- `sp` - equivalent of vector shapefiles in a GIS

    - Polygons, Points, Lines
    - Not always fast, but essential to have

## Key packages for spatial simulation - page 2/2
    
- `data.table`

    - For `data.frame` type data (i.e., columns of data)
    - Very fast when object gets large, but is actually slower if the `data.frame` is small (<100,000 rows)
    
- `SpaDES` -- many functions -- will be moved into a separate package soon ("spatsim")

- `Rcpp`

    - R interface to C++ . When you need something fast, and you can't get it fast enough with existing tools/packages, you can create your own (we will not go further into this here)

## What we will do here

- Each person should decide which tool is the most useful to them
- We will link to external tutorials to work through 
- Each person will either choose **one** package to try, or work through SpaDES functions

# Working with spatial data

## "Vectorization"

- This is at the core of making R fast. If you don't do this, then it is probably not useful to use R as a simulation engine.

```{r vectorization}
# Instead of 
a <- vector()
for (i in 1:1000) {
  a[i] <- rnorm(1)
}

# use vectorized version, which is built into the functions
a <- rnorm(1000000)

```

## Vectors and Matrices

- These are
- fast numerical operations


## Predictive Ecology blog posts about R speed

- [Is R Fast Enough, part 1, The Mean](http://predictiveecology.org/2015/04/23/Is-R-fast-enough-01.html)
- [Is R Fast Enough, part 2, Sorting](http://predictiveecology.org/2015/04/28/Is-R-fast-enough-02.html)
- [Is R Fast Enough, part 3, Fibonacci](http://predictiveecology.org/2015/05/06/Is-R-fast-enough-03.html)
- [Is R Fast Enough, part 4, Loops](http://predictiveecology.org/2015/09/04/Is-R-fast-enough-04.html)

## raster

- tutorials 

    - [NEON](http://neondataskills.org/R/Raster-Data-In-R/)
    - [geoscripting-wur](http://geoscripting-wur.github.io/IntroToRaster/)
    - [Official vignette](https://cran.r-project.org/web/packages/raster/vignettes/Raster.pdf)
    - [Using R as a GIS](https://pakillo.github.io/R-GIS-tutorial/)
    - many more available


## The `data.table` package

From every `data.table` user ever:

> WOW that's fast!

```{r install-data.table, echo=TRUE, eval=FALSE}
install.packages('data.table')
```

## SpaDES functions

- These are all potentially useful for building spatio-temporal models
```{r SpaDES functions}
?spread
?move
?cir
?adj
?distanceFromEachPoint

```

## The `Rcpp` package

From every `Rcpp` user ever:

> WOW! Just wow.

```{r install-rcpp, echo=TRUE, eval=FALSE}
install.packages('Rcpp')
```


## Profiling and Benchmarking, page 1/3

- In general, the usual claim is to worry about 'execution speed later'
- This is not 100% true with R
- If you use vectorization (no or few loops), and these packages listed here, then you will have a good start
- AFTER that, then you can use 2 great tools:

    - `profvis` package (built into the latest Rstudio Release candidates, NOT the normal Rstudio yet)
    - `microbenchmark` package

## Profiling and Benchmarking, page 2/3

```{r microbenchmark, eval=TRUE, echo=TRUE}

microbenchmark::microbenchmark(loop = { a <- vector()
                                 for (i in 1:1000) a[i] <- rnorm(1)
}, vectorized = {                              a <- rnorm(1000) } 
)

```


## Profiling and Benchmarking, page 3/3

If you have Rstudio version 0.99.1208 or greater (or near that version number), then it has profiling as a menu item.

- alternatively, we wrap any block of code with `profvis`

- This can be a spades call, so it will show you the entire model:

```{r profiling, eval=TRUE, echo=TRUE}
profvis::profvis({a <- rnorm(10000000)})
```

## Profiling spades call

Try it:

```{r profiling spades, eval=FALSE, echo=TRUE}
profvis::profvis({spades(mySim)})

```
## When to profile

- First, you should have started building your code with the packages we have discussed
- It will be too late if you have loops in your code, and you are ready to profile to improve it

If you have used these tools, then:

- When you have mostly finished whatever we are coding
- Don't ever start making code more efficient until you have profiled
- it is almost impossible to tell which bits are the slow parts, without profiling or benchmarking

## Strategies for profiling

- Can do an entire spades model call
- Can pinpoint specific functions
- Can test alternative ways of implementing the same thing
